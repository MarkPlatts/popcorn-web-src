{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "When we are using a preconfigured word embeddings model and convolution or recurrent neural networks the data cleaning process is quite different from the one applied when preparing data for traditional machine learning models.\n",
    "\n",
    "When we are fitting an RNN we often want to keep the text in it's original format because it contains important information. For example, we often don't want to get rid of stopwords because they might tell us something about surround words such as whether a word is a noun or a verb. Instead, the main goal of cleaning when using an RNN with pre-trained word embeddings is getting the vocabulary used in the dataset as similar as possible to that used in the embeddings model; this is important because unless the word can be transformed into a vector the model cannot use it. The specifics of how this is done depends on the embedding. We will be using the glove.840B.300d pre-trained vectors (https://nlp.stanford.edu/projects/glove/). The general approach is approximately the same for other word embeddings.\n",
    "\n",
    "What we will do here is go through an iterative process that starts by looking at how much of the data can be represented by the pre-trained word vectors and identifying the the most common tokens that can't be represented. We then write a function that cleans the data so that these tokens are modified to text that can be represented. We then repeat this process by checking which tokens can't be represented and tweaking our cleaning function until we are happy that it cleans the data well enough. This function can then be used as part of our pipeline well modelling.\n",
    "\n",
    "Note: Note that tqdm is a nice little package that you can use to output a progress bar when you perform a loop. For more information https://pypi.org/project/tqdm/. In most cases I ended up not using this in this code because it doesn't display well in a jupyter notebook, but when working on your own code it can be useful to see that your program is progressing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../../data/labeledTrainData.tsv\", sep='\\t', encoding='utf-8')\n",
    "print(\"Train shape : \",train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0282ff8c9c01d42fab708463d6d1b1b181069d12"
   },
   "outputs": [],
   "source": [
    "def build_vocab(sentences, verbose =  False):\n",
    "    \"\"\"\n",
    "    :param sentences: list of list of words\n",
    "    :return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first things we need to do is to create a dictionary which for each occuring word records the number of occurances. We can then compare this with the words in the embedding that we are using to see if it exists and calculate statistics on the percentage of words that do and don't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c22d00e87cf53e9544a56250eee695e55fba91bb"
   },
   "outputs": [],
   "source": [
    "sentences = train[\"review\"].progress_apply(lambda x: x.split()).values\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will load the Glove embeddings file into a dict where the word is the key and the vector is the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f8beb4df42d143b83fa35f85d70d39fd449a7e3b"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE_GLOVE = '../../embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embed_glove = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE_GLOVE, encoding='latin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the percentage of the vocabulary and of all the text are calculated so we can see how well the text in its current format is covered by the pre-trained word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "951f289357dfa9a898b77606ddfd8713f493263e"
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def check_coverage(vocab,embed_glove):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    k = 0\n",
    "    i = 0\n",
    "    for word in vocab:\n",
    "        try:\n",
    "            a[word] = embed_glove[word]\n",
    "            k += vocab[word]\n",
    "        except:\n",
    "\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x\n",
    "\n",
    "oov = check_coverage(vocab,embed_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the top ten words that do not occur in the Glove Embedding we should get some idea how the data might need cleaning to fit the embeddings better and get a better coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eaf66355bf715e2c92e68c1dfa45f87daf680f2a"
   },
   "outputs": [],
   "source": [
    "print(oov[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "24a062cc9744431385f296c114d7232c77c3d1eb"
   },
   "source": [
    "The most obvious thing here is that the data includes some html. We can get rid of that using the fantastic beautifulsoup package. To do this we will create a clean_text function that is passed each review and returns back a cleaned version of the review. We can then run code similar to above to see how well the pre-trained vectors covers the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f30f31898115f985bfc1fa3bc3b5287ebb2339d4"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    text = str(text)\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    return( text )\n",
    "\n",
    "def run_cleaning_output_summary(fun):\n",
    "    cleaned = train[\"review\"].apply(lambda x: fun(x))\n",
    "    sentences = cleaned.apply(lambda x: x.split())\n",
    "    vocab = build_vocab(sentences)\n",
    "    oov = check_coverage(vocab,embed_glove)\n",
    "    print(\"Top 20 most common words not found in Glove embedding:\")\n",
    "    print(oov[:30])\n",
    "    \n",
    "run_cleaning_output_summary(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After printing out the 20 most common words that aren't in the embedding we can see that punctuation is causing a significant problem. First let us see if punctuation has a corresponding Glove vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Check to see which punctuation if any have vectors:\")\n",
    "for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "    print(punct, \" : \", punct in embed_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost everything does. If we surround punctuation by a space then punctuation will be picked up as a token and converted into it's corresponding vector. Let's try it and see what happens. To do this we modify the clean_text function and run the above code again to see if the coverage improves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    text = str(text)\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~“”’’”“':\n",
    "        text = text.replace(punct, f' {punct} ')\n",
    "    \n",
    "    return text\n",
    "\n",
    "run_cleaning_output_summary(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bd3eee8bdcb01b9920ef6a71c6481dbed159c166"
   },
   "source": [
    "We've gone from being able to map 89.17% of the words to vectors to 99.77% of the words which is a great improvement and we are definitely getting to the point where cleaning the data further is not worth the effort. So lets try one last iteration on this data cleaning process.\n",
    "\n",
    "The easiest words to convert here are foreign words containing accents. We simply need to replace the accented letter. I also replaced \\x96 with a - and the ’ with a ' although it's not clear whether it is a good idea to do this without seeing how it impacts model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    text = str(text)\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~“”’’”“':\n",
    "        text = text.replace(punct, f' {punct} ')\n",
    "        \n",
    "    text = text.replace(\"cliché\", 'cliche')   \n",
    "    text = text.replace(\"clichés\", 'cliches')\n",
    "    text = text.replace(\"cliched\", 'cliched')\n",
    "    text = text.replace(\"fiancé\", 'fiance')\n",
    "    text = text.replace(\"fiancée\", 'fiancee')\n",
    "    text = text.replace(\"café\", 'cafe')\n",
    "    text = text.replace(\"matinée\", 'matinee')\n",
    "    text = text.replace(\"naïve\", 'naive')\n",
    "    text = text.replace(\"José\", 'Jose')\n",
    "    text = text.replace(\"risqué\", 'risque')\n",
    "    \n",
    "    text = text.replace('\\x96', '-')\n",
    "    \n",
    "    text = text.replace('’', \"'\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "run_cleaning_output_summary(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could go further but now we have found embeddings for 99.8% we should definitely question whether the extra effort is worth it.\n",
    "\n",
    "We now have a function that can be used to clean the data ready to be used by our RNN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
